- BERT Notes:
    - Learning rate for BERT needs to be low (below 0.001) use 1e-5
    - In low data scenarios try freezing the gradients of the model. For example just fine tune the last layer, last 2 layers, etc. This amounts to setting requires_grad=False for the layers you donâ€™t want to update during training.

- Dataset Notes:
    - Dataset looks like it may be too small (only 691 examples, and each is unique classifcation class):
        - Random Forest Baseline model is also having zero accuracy, which points to data issues.
        - Augment the dataset with synthetic data for each WCIRB classification code
        - Augmenting: Synonym replacement, running it through a circle of translation models, using an LLM to generate text that would fall under a specific label
    - Where to store the bigger dataset (local machine will not have enough space for thousands of examples)
    
